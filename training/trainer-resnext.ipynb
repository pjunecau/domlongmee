{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014178,
     "end_time": "2021-03-29T02:04:01.505830",
     "exception": false,
     "start_time": "2021-03-29T02:04:01.491652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training:\n",
    "> #### Main ideas:\n",
    "* StratifiedKFold cross validation strategy.\n",
    "* Bi-Tempered loss function to handle noisy labels.\n",
    "* Self-distillation using soft labels.\n",
    "\n",
    "More info about the model training: [How to train ML models with mislabeled data](https://aminey.medium.com/how-to-train-ml-models-with-mislabeled-data-cf4bb353b3d9?sk=9f4ce905cd5c4f2d86ec3bf7b93d024c).\n",
    "\n",
    "\n",
    "## Datasets\n",
    "You can either download the datasets below and train the model locally or create a kaggle notebook and attach the datasets to the notebook.\n",
    "\n",
    "\n",
    "* The datasets are available at https://www.kaggle.com/c/cassava-leaf-disease-classification/data\n",
    "\n",
    "* Soft labels dataset: https://www.kaggle.com/nickuzmenkov/cassava-leaf-disease-soft-targets-09-model\n",
    "\n",
    "* The training notebook was forked and modified from https://www.kaggle.com/yasufuminakama/cassava-resnext50-32x4d-starter-training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012779,
     "end_time": "2021-03-29T02:04:01.531641",
     "exception": false,
     "start_time": "2021-03-29T02:04:01.518862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:01.569364Z",
     "iopub.status.busy": "2021-03-29T02:04:01.568784Z",
     "iopub.status.idle": "2021-03-29T02:04:05.880548Z",
     "shell.execute_reply": "2021-03-29T02:04:05.879995Z"
    },
    "papermill": {
     "duration": 4.336166,
     "end_time": "2021-03-29T02:04:05.880659",
     "exception": false,
     "start_time": "2021-03-29T02:04:01.544493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # specify GPUs locally\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013132,
     "end_time": "2021-03-29T02:04:05.907435",
     "exception": false,
     "start_time": "2021-03-29T02:04:05.894303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:05.941147Z",
     "iopub.status.busy": "2021-03-29T02:04:05.940615Z",
     "iopub.status.idle": "2021-03-29T02:04:05.970754Z",
     "shell.execute_reply": "2021-03-29T02:04:05.970306Z"
    },
    "papermill": {
     "duration": 0.050194,
     "end_time": "2021-03-29T02:04:05.970840",
     "exception": false,
     "start_time": "2021-03-29T02:04:05.920646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "#Path to image folder\n",
    "TRAIN_PATH = '../input/cassava-leaf-disease-classification/train_images'\n",
    "\n",
    "#Train dataset\n",
    "train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "train= pd.get_dummies(train, columns=['label'])\n",
    "\n",
    "#Soft labels dataset\n",
    "soft_labels='../input/cassava-leaf-disease-soft-targets-09-model/soft_targets_2020.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012912,
     "end_time": "2021-03-29T02:04:05.996989",
     "exception": false,
     "start_time": "2021-03-29T02:04:05.984077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.033040Z",
     "iopub.status.busy": "2021-03-29T02:04:06.031207Z",
     "iopub.status.idle": "2021-03-29T02:04:06.033660Z",
     "shell.execute_reply": "2021-03-29T02:04:06.034159Z"
    },
    "papermill": {
     "duration": 0.023929,
     "end_time": "2021-03-29T02:04:06.034275",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.010346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug=False\n",
    "    apex=False\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model_name='resnext50_32x4d'\n",
    "    size=328\n",
    "    epochs=10\n",
    "    T_0=10 # CosineAnnealingWarmRestarts\n",
    "    lr=1e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=32\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    target_size=5\n",
    "    target_col='label'\n",
    "    n_fold=5\n",
    "    trn_fold=[0]\n",
    "    train=True\n",
    "    smoothing=0.05\n",
    "    t1=0.3 # bi-tempered-loss temperature 1 parameter\n",
    "    t2=1.0 # bi-tempered-loss temperature 2 parameter\n",
    "    solf_label=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015441,
     "end_time": "2021-03-29T02:04:06.065034",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.049593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.109722Z",
     "iopub.status.busy": "2021-03-29T02:04:06.108772Z",
     "iopub.status.idle": "2021-03-29T02:04:06.119307Z",
     "shell.execute_reply": "2021-03-29T02:04:06.118779Z"
    },
    "papermill": {
     "duration": 0.038567,
     "end_time": "2021-03-29T02:04:06.119414",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.080847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_score(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01641,
     "end_time": "2021-03-29T02:04:06.153271",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.136861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross-Validation\n",
    "StratifiedKFold cross validation to split the 5 classes equally between the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.197064Z",
     "iopub.status.busy": "2021-03-29T02:04:06.196037Z",
     "iopub.status.idle": "2021-03-29T02:04:06.234670Z",
     "shell.execute_reply": "2021-03-29T02:04:06.234167Z"
    },
    "papermill": {
     "duration": 0.063552,
     "end_time": "2021-03-29T02:04:06.234787",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.171235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_col])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013279,
     "end_time": "2021-03-29T02:04:06.263397",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.250118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "Dataset with the `soft_labels_filename` parameter for self-distillation. To blend the original one hot encoded labels with the soft labels (out of folds predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.304289Z",
     "iopub.status.busy": "2021-03-29T02:04:06.303385Z",
     "iopub.status.idle": "2021-03-29T02:04:06.307924Z",
     "shell.execute_reply": "2021-03-29T02:04:06.307461Z"
    },
    "papermill": {
     "duration": 0.030301,
     "end_time": "2021-03-29T02:04:06.308009",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.277708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, soft_labels_filename=None, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        if soft_labels_filename == \"\":\n",
    "            print(\"soft_labels is None\")\n",
    "            self.soft_labels = None\n",
    "        else:\n",
    "            self.soft_labels = pd.read_csv(soft_labels_filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Read image\n",
    "        file_name = self.file_names[index]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        # Soft label for self-distillation\n",
    "        if self.soft_labels is not None:\n",
    "            label = torch.FloatTensor(\n",
    "                (self.df.iloc[index, 1:].values * 0.7).astype(np.float16)\n",
    "                + (self.soft_labels.iloc[index, 1:].values * 0.3).astype(np.float16)\n",
    "            )\n",
    "        else:\n",
    "            label = torch.FloatTensor(self.df.iloc[index, 1:].values.astype(np.float16))\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014278,
     "end_time": "2021-03-29T02:04:06.336100",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.321822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.374666Z",
     "iopub.status.busy": "2021-03-29T02:04:06.373983Z",
     "iopub.status.idle": "2021-03-29T02:04:06.376995Z",
     "shell.execute_reply": "2021-03-29T02:04:06.376528Z"
    },
    "papermill": {
     "duration": 0.026527,
     "end_time": "2021-03-29T02:04:06.377081",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.350554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return A.Compose([\n",
    "            #Resize(CFG.size, CFG.size),\n",
    "            A.RandomResizedCrop(CFG.size, CFG.size),\n",
    "            A.Transpose(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            A.CoarseDropout(p=0.5),\n",
    "            A.Cutout(p=0.5),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return A.Compose([\n",
    "            A.Resize(CFG.size, CFG.size),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013492,
     "end_time": "2021-03-29T02:04:06.404182",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.390690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.438034Z",
     "iopub.status.busy": "2021-03-29T02:04:06.437297Z",
     "iopub.status.idle": "2021-03-29T02:04:06.439565Z",
     "shell.execute_reply": "2021-03-29T02:04:06.440084Z"
    },
    "papermill": {
     "duration": 0.022206,
     "end_time": "2021-03-29T02:04:06.440180",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.417974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomResNext(nn.Module):\n",
    "    def __init__(self, model_name='resnext50_32x4d', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(n_features, CFG.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01439,
     "end_time": "2021-03-29T02:04:06.468261",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.453871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bi-Tempered-Loss\n",
    "Bi-Tempered loss function to train ML models with noisy data, introduced by google AI in their blog: https://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html\n",
    "\n",
    "Code available in the google-ai github: https://github.com/google/bi-tempered-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.527955Z",
     "iopub.status.busy": "2021-03-29T02:04:06.507125Z",
     "iopub.status.idle": "2021-03-29T02:04:06.542578Z",
     "shell.execute_reply": "2021-03-29T02:04:06.542138Z"
    },
    "papermill": {
     "duration": 0.058435,
     "end_time": "2021-03-29T02:04:06.542665",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.484230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u'.\"\"\"\n",
    "    if t==1.0:\n",
    "        return u.log()\n",
    "    else:\n",
    "        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u'.\"\"\"\n",
    "    if t==1:\n",
    "        return u.exp()\n",
    "    else:\n",
    "        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same shape as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = torch.sum(\n",
    "                exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "        normalized_activations = normalized_activations_step_0 * \\\n",
    "                logt_partition.pow(1.0-t)\n",
    "\n",
    "    logt_partition = torch.sum(\n",
    "            exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "    return normalization_constants\n",
    "\n",
    "def compute_normalization_binary_search(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (< 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations = activations - mu\n",
    "\n",
    "    effective_dim = \\\n",
    "        torch.sum(\n",
    "                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n",
    "            dim=-1, keepdim=True).to(activations.dtype)\n",
    "\n",
    "    shape_partition = activations.shape[:-1] + (1,)\n",
    "    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n",
    "    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = (upper + lower)/2.0\n",
    "        sum_probs = torch.sum(\n",
    "                exp_t(normalized_activations - logt_partition, t),\n",
    "                dim=-1, keepdim=True)\n",
    "        update = (sum_probs < 1.0).to(activations.dtype)\n",
    "        lower = torch.reshape(\n",
    "                lower * update + (1.0-update) * logt_partition,\n",
    "                shape_partition)\n",
    "        upper = torch.reshape(\n",
    "                upper * (1.0 - update) + update * logt_partition,\n",
    "                shape_partition)\n",
    "\n",
    "    logt_partition = (upper + lower)/2.0\n",
    "    return logt_partition + mu\n",
    "\n",
    "class ComputeNormalization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, activations, t, num_iters):\n",
    "        if t < 1.0:\n",
    "            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n",
    "        else:\n",
    "            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "        ctx.save_for_backward(activations, normalization_constants)\n",
    "        ctx.t=t\n",
    "        return normalization_constants\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        activations, normalization_constants = ctx.saved_tensors\n",
    "        t = ctx.t\n",
    "        normalized_activations = activations - normalization_constants \n",
    "        probabilities = exp_t(normalized_activations, t)\n",
    "        escorts = probabilities.pow(t)\n",
    "        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n",
    "        grad_input = escorts * grad_output\n",
    "        \n",
    "        return grad_input, None, None\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example. \n",
    "    Backward pass is implemented.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    return ComputeNormalization.apply(activations, t, num_iters)\n",
    "\n",
    "def tempered_sigmoid(activations, t, num_iters = 5):\n",
    "    \"\"\"Tempered sigmoid function.\n",
    "    Args:\n",
    "      activations: Activations for the positive class for binary classification.\n",
    "      t: Temperature tensor > 0.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n",
    "    return internal_probabilities[..., 0]\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature > 1.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    if t == 1.0:\n",
    "        return activations.softmax(dim=-1)\n",
    "\n",
    "    normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "def bi_tempered_binary_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing = 0.0,\n",
    "        num_iters=5,\n",
    "        reduction='mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered binary logistic loss.\n",
    "    Args:\n",
    "      activations: A tensor containing activations for class 1.\n",
    "      labels: A tensor with shape as activations, containing probabilities for class 1\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_labels = torch.stack([labels.to(activations.dtype),\n",
    "        1.0 - labels.to(activations.dtype)],\n",
    "        dim=-1)\n",
    "    return bi_tempered_logistic_loss(internal_activations, \n",
    "            internal_labels,\n",
    "            t1,\n",
    "            t2,\n",
    "            label_smoothing = label_smoothing,\n",
    "            num_iters = num_iters,\n",
    "            reduction = reduction)\n",
    "\n",
    "def bi_tempered_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing=0.0,\n",
    "        num_iters=5,\n",
    "        reduction = 'mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered Logistic Loss.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      labels: A tensor with shape and dtype as activations (onehot), \n",
    "        or a long tensor of one dimension less than activations (pytorch standard)\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n",
    "      num_iters: Number of iterations to run the method. Default 5.\n",
    "      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n",
    "        ``'none'``: No reduction is applied, return shape is shape of\n",
    "        activations without the last dimension.\n",
    "        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n",
    "        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(labels.shape)<len(activations.shape): #not one-hot\n",
    "        labels_onehot = torch.zeros_like(activations)\n",
    "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
    "    else:\n",
    "        labels_onehot = labels\n",
    "\n",
    "    if label_smoothing > 0:\n",
    "        num_classes = labels_onehot.shape[-1]\n",
    "        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n",
    "                * labels_onehot + \\\n",
    "                label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
    "            - labels_onehot * log_t(probabilities, t1) \\\n",
    "            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
    "            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
    "    loss_values = loss_values.sum(dim = -1) #sum over classes\n",
    "\n",
    "    if reduction == 'none':\n",
    "        return loss_values\n",
    "    if reduction == 'sum':\n",
    "        return loss_values.sum()\n",
    "    if reduction == 'mean':\n",
    "        return loss_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.577098Z",
     "iopub.status.busy": "2021-03-29T02:04:06.576443Z",
     "iopub.status.idle": "2021-03-29T02:04:06.579336Z",
     "shell.execute_reply": "2021-03-29T02:04:06.578935Z"
    },
    "papermill": {
     "duration": 0.022831,
     "end_time": "2021-03-29T02:04:06.579428",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.556597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiTemperedLogisticLoss(nn.Module): \n",
    "    def __init__(self, t1, t2, smoothing=0.0): \n",
    "        super(BiTemperedLogisticLoss, self).__init__() \n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, logit_label, truth_label):\n",
    "        loss_label = bi_tempered_logistic_loss(\n",
    "            logit_label, truth_label,\n",
    "            t1=self.t1, t2=self.t2,\n",
    "            label_smoothing=self.smoothing,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        loss_label = loss_label.mean()\n",
    "        return loss_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01378,
     "end_time": "2021-03-29T02:04:06.607267",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.593487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.662885Z",
     "iopub.status.busy": "2021-03-29T02:04:06.647899Z",
     "iopub.status.idle": "2021-03-29T02:04:06.665427Z",
     "shell.execute_reply": "2021-03-29T02:04:06.665018Z"
    },
    "papermill": {
     "duration": 0.044264,
     "end_time": "2021-03-29T02:04:06.665510",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.621246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(images)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        else:\n",
    "            loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  #'LR: {lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   grad_norm=grad_norm,\n",
    "                   #lr=scheduler.get_lr()[0],\n",
    "                   ))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        \n",
    "        preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013978,
     "end_time": "2021-03-29T02:04:06.693969",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.679991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.744789Z",
     "iopub.status.busy": "2021-03-29T02:04:06.734953Z",
     "iopub.status.idle": "2021-03-29T02:04:06.747390Z",
     "shell.execute_reply": "2021-03-29T02:04:06.746975Z"
    },
    "papermill": {
     "duration": 0.039353,
     "end_time": "2021-03-29T02:04:06.747478",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.708125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train loop\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # loader\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, soft_labels_filename=soft_labels,\n",
    "                                 transform=get_transforms(data='train'))\n",
    "    valid_dataset = TrainDataset(valid_folds, soft_labels_filename=soft_labels, \n",
    "                                 transform=get_transforms(data='valid'))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    #Scheduler\n",
    "    def get_scheduler(optimizer):\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    #Model\n",
    "    model = CustomResNext(CFG.model_name, pretrained=True)    \n",
    "    model.to(device)\n",
    "\n",
    "    #Optimzer, scheduler and loss\n",
    "    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = BiTemperedLogisticLoss(t1=CFG.t1, t2=CFG.t2, smoothing=CFG.smoothing)\n",
    "    #criterion = CrossEntropyLossOneHot\n",
    "\n",
    "\n",
    "    # loop \n",
    "    LOGGER.info(f'Criterion: {criterion}')\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        #train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        #eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        #Read the labels for validation\n",
    "        if CFG.solf_label:\n",
    "            #Reverse One hot encoding\n",
    "            valid_labels=valid_folds.rename(columns={'label_0':0,'label_1':1,'label_2':2,'label_3':3,'label_4':4})\n",
    "            valid_labels['label']=valid_labels.iloc[:,1:].idxmax(axis=1)\n",
    "            \n",
    "            #Compute accuracy with original labels\n",
    "            valid_labels = valid_labels[CFG.target_col].values\n",
    "        else:\n",
    "            valid_labels = valid_folds[CFG.target_col].values\n",
    "            \n",
    "        #Save oof labels\n",
    "        \n",
    "        \n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        \n",
    "        # scoring\n",
    "        score = get_score(valid_labels, preds.argmax(1))\n",
    "        del valid_labels\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Accuracy: {score}')\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "    \n",
    "    check_point = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "    valid_folds[[str(c) for c in range(5)]] = check_point['preds']\n",
    "    valid_folds['preds'] = check_point['preds'].argmax(1)\n",
    "    \n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.781688Z",
     "iopub.status.busy": "2021-03-29T02:04:06.781042Z",
     "iopub.status.idle": "2021-03-29T02:04:06.783781Z",
     "shell.execute_reply": "2021-03-29T02:04:06.784174Z"
    },
    "papermill": {
     "duration": 0.022517,
     "end_time": "2021-03-29T02:04:06.784286",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.761769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(): \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(folds, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T02:04:06.817589Z",
     "iopub.status.busy": "2021-03-29T02:04:06.816769Z",
     "iopub.status.idle": "2021-03-29T03:05:53.416896Z",
     "shell.execute_reply": "2021-03-29T03:05:53.417346Z"
    },
    "papermill": {
     "duration": 3706.618851,
     "end_time": "2021-03-29T03:05:53.417515",
     "exception": false,
     "start_time": "2021-03-29T02:04:06.798664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50_32x4d_ra-d733960d.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d_ra-d733960d.pth\n",
      "Criterion: BiTemperedLogisticLoss()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/534] Data 2.400 (2.400) Elapsed 0m 4s (remain 43m 1s) Loss: 0.2933(0.2933) Grad: 1.8009  \n",
      "Epoch: [1][100/534] Data 0.000 (0.024) Elapsed 1m 3s (remain 4m 33s) Loss: 0.1039(0.1375) Grad: 1.1592  \n",
      "Epoch: [1][200/534] Data 0.000 (0.012) Elapsed 2m 3s (remain 3m 24s) Loss: 0.1386(0.1249) Grad: 1.1669  \n",
      "Epoch: [1][300/534] Data 0.000 (0.008) Elapsed 3m 3s (remain 2m 21s) Loss: 0.1113(0.1181) Grad: 1.0025  \n",
      "Epoch: [1][400/534] Data 0.005 (0.006) Elapsed 4m 2s (remain 1m 20s) Loss: 0.1223(0.1135) Grad: 1.2078  \n",
      "Epoch: [1][500/534] Data 0.000 (0.005) Elapsed 5m 1s (remain 0m 19s) Loss: 0.1264(0.1098) Grad: 1.2845  \n",
      "Epoch: [1][533/534] Data 0.000 (0.005) Elapsed 5m 21s (remain 0m 0s) Loss: 0.0658(0.1086) Grad: 0.8226  \n",
      "EVAL: [0/134] Data 1.467 (1.467) Elapsed 0m 1s (remain 3m 34s) Loss: 0.1126(0.1126) \n",
      "EVAL: [100/134] Data 0.693 (0.239) Elapsed 0m 39s (remain 0m 12s) Loss: 0.0896(0.0875) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1086  avg_val_loss: 0.0879  time: 373s\n",
      "Epoch 1 - Accuracy: 0.8378504672897197\n",
      "Epoch 1 - Save Best Score: 0.8379 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.233) Elapsed 0m 51s (remain 0m 0s) Loss: 0.1345(0.0879) \n",
      "Epoch: [2][0/534] Data 1.815 (1.815) Elapsed 0m 2s (remain 21m 57s) Loss: 0.0741(0.0741) Grad: 0.8962  \n",
      "Epoch: [2][100/534] Data 0.000 (0.019) Elapsed 1m 2s (remain 4m 28s) Loss: 0.0945(0.0915) Grad: 0.9347  \n",
      "Epoch: [2][200/534] Data 0.000 (0.010) Elapsed 2m 1s (remain 3m 21s) Loss: 0.0796(0.0912) Grad: 0.7225  \n",
      "Epoch: [2][300/534] Data 0.000 (0.007) Elapsed 3m 1s (remain 2m 20s) Loss: 0.0597(0.0901) Grad: 0.7590  \n",
      "Epoch: [2][400/534] Data 0.000 (0.005) Elapsed 4m 0s (remain 1m 19s) Loss: 0.0915(0.0885) Grad: 0.8910  \n",
      "Epoch: [2][500/534] Data 0.001 (0.004) Elapsed 5m 0s (remain 0m 19s) Loss: 0.0686(0.0879) Grad: 0.8598  \n",
      "Epoch: [2][533/534] Data 0.000 (0.004) Elapsed 5m 19s (remain 0m 0s) Loss: 0.0752(0.0879) Grad: 0.7848  \n",
      "EVAL: [0/134] Data 1.321 (1.321) Elapsed 0m 1s (remain 3m 16s) Loss: 0.1130(0.1130) \n",
      "EVAL: [100/134] Data 0.342 (0.226) Elapsed 0m 38s (remain 0m 12s) Loss: 0.1062(0.0910) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0879  avg_val_loss: 0.0904  time: 369s\n",
      "Epoch 2 - Accuracy: 0.8558411214953271\n",
      "Epoch 2 - Save Best Score: 0.8558 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.217) Elapsed 0m 49s (remain 0m 0s) Loss: 0.1184(0.0904) \n",
      "Epoch: [3][0/534] Data 1.895 (1.895) Elapsed 0m 2s (remain 22m 42s) Loss: 0.0857(0.0857) Grad: 0.8318  \n",
      "Epoch: [3][100/534] Data 0.000 (0.020) Elapsed 1m 2s (remain 4m 28s) Loss: 0.0599(0.0799) Grad: 0.3873  \n",
      "Epoch: [3][200/534] Data 0.000 (0.010) Elapsed 2m 1s (remain 3m 21s) Loss: 0.0690(0.0821) Grad: 0.5025  \n",
      "Epoch: [3][300/534] Data 0.000 (0.007) Elapsed 3m 0s (remain 2m 20s) Loss: 0.0765(0.0824) Grad: 0.8225  \n",
      "Epoch: [3][400/534] Data 0.000 (0.005) Elapsed 4m 0s (remain 1m 19s) Loss: 0.1214(0.0830) Grad: 1.0834  \n",
      "Epoch: [3][500/534] Data 0.000 (0.004) Elapsed 4m 59s (remain 0m 19s) Loss: 0.0687(0.0832) Grad: 0.6559  \n",
      "Epoch: [3][533/534] Data 0.000 (0.004) Elapsed 5m 19s (remain 0m 0s) Loss: 0.0725(0.0832) Grad: 0.5923  \n",
      "EVAL: [0/134] Data 1.354 (1.354) Elapsed 0m 1s (remain 3m 21s) Loss: 0.0985(0.0985) \n",
      "EVAL: [100/134] Data 0.485 (0.220) Elapsed 0m 37s (remain 0m 12s) Loss: 0.0782(0.0792) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0832  avg_val_loss: 0.0792  time: 368s\n",
      "Epoch 3 - Accuracy: 0.8658878504672897\n",
      "Epoch 3 - Save Best Score: 0.8659 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.211) Elapsed 0m 48s (remain 0m 0s) Loss: 0.1102(0.0792) \n",
      "Epoch: [4][0/534] Data 2.642 (2.642) Elapsed 0m 3s (remain 29m 52s) Loss: 0.0336(0.0336) Grad: 0.3723  \n",
      "Epoch: [4][100/534] Data 0.000 (0.027) Elapsed 1m 3s (remain 4m 30s) Loss: 0.0562(0.0777) Grad: 0.5306  \n",
      "Epoch: [4][200/534] Data 0.000 (0.014) Elapsed 2m 2s (remain 3m 23s) Loss: 0.1069(0.0774) Grad: 0.8286  \n",
      "Epoch: [4][300/534] Data 0.000 (0.009) Elapsed 3m 2s (remain 2m 21s) Loss: 0.0971(0.0772) Grad: 0.6754  \n",
      "Epoch: [4][400/534] Data 0.000 (0.007) Elapsed 4m 1s (remain 1m 20s) Loss: 0.0919(0.0776) Grad: 0.8855  \n",
      "Epoch: [4][500/534] Data 0.000 (0.006) Elapsed 5m 1s (remain 0m 19s) Loss: 0.1096(0.0779) Grad: 0.9963  \n",
      "Epoch: [4][533/534] Data 0.000 (0.006) Elapsed 5m 20s (remain 0m 0s) Loss: 0.0955(0.0781) Grad: 0.6026  \n",
      "EVAL: [0/134] Data 1.683 (1.683) Elapsed 0m 1s (remain 4m 5s) Loss: 0.1012(0.1012) \n",
      "EVAL: [100/134] Data 0.753 (0.228) Elapsed 0m 38s (remain 0m 12s) Loss: 0.0788(0.0777) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0781  avg_val_loss: 0.0774  time: 370s\n",
      "Epoch 4 - Accuracy: 0.8707943925233644\n",
      "Epoch 4 - Save Best Score: 0.8708 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.216) Elapsed 0m 49s (remain 0m 0s) Loss: 0.1101(0.0774) \n",
      "Epoch: [5][0/534] Data 1.898 (1.898) Elapsed 0m 2s (remain 22m 46s) Loss: 0.0818(0.0818) Grad: 0.6481  \n",
      "Epoch: [5][100/534] Data 0.000 (0.020) Elapsed 1m 2s (remain 4m 29s) Loss: 0.1109(0.0751) Grad: 0.9106  \n",
      "Epoch: [5][200/534] Data 0.000 (0.010) Elapsed 2m 2s (remain 3m 23s) Loss: 0.0791(0.0760) Grad: 0.5616  \n",
      "Epoch: [5][300/534] Data 0.000 (0.007) Elapsed 3m 2s (remain 2m 21s) Loss: 0.0607(0.0762) Grad: 0.5065  \n",
      "Epoch: [5][400/534] Data 0.000 (0.005) Elapsed 4m 1s (remain 1m 20s) Loss: 0.0667(0.0755) Grad: 0.5711  \n",
      "Epoch: [5][500/534] Data 0.000 (0.004) Elapsed 5m 1s (remain 0m 19s) Loss: 0.0788(0.0754) Grad: 0.7910  \n",
      "Epoch: [5][533/534] Data 0.000 (0.004) Elapsed 5m 20s (remain 0m 0s) Loss: 0.0761(0.0756) Grad: 0.7487  \n",
      "EVAL: [0/134] Data 1.357 (1.357) Elapsed 0m 1s (remain 3m 20s) Loss: 0.0919(0.0919) \n",
      "EVAL: [100/134] Data 0.870 (0.221) Elapsed 0m 37s (remain 0m 12s) Loss: 0.0857(0.0769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0756  avg_val_loss: 0.0778  time: 370s\n",
      "Epoch 5 - Accuracy: 0.8710280373831776\n",
      "Epoch 5 - Save Best Score: 0.8710 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.212) Elapsed 0m 48s (remain 0m 0s) Loss: 0.1227(0.0778) \n",
      "Epoch: [6][0/534] Data 1.929 (1.929) Elapsed 0m 2s (remain 22m 29s) Loss: 0.0747(0.0747) Grad: 0.5342  \n",
      "Epoch: [6][100/534] Data 0.000 (0.020) Elapsed 1m 2s (remain 4m 28s) Loss: 0.0811(0.0758) Grad: 0.6503  \n",
      "Epoch: [6][200/534] Data 0.000 (0.010) Elapsed 2m 2s (remain 3m 22s) Loss: 0.0569(0.0752) Grad: 0.5478  \n",
      "Epoch: [6][300/534] Data 0.000 (0.007) Elapsed 3m 1s (remain 2m 20s) Loss: 0.0391(0.0748) Grad: 0.5272  \n",
      "Epoch: [6][400/534] Data 0.002 (0.005) Elapsed 4m 1s (remain 1m 19s) Loss: 0.0641(0.0741) Grad: 0.5892  \n",
      "Epoch: [6][500/534] Data 0.005 (0.005) Elapsed 5m 0s (remain 0m 19s) Loss: 0.0771(0.0737) Grad: 0.5781  \n",
      "Epoch: [6][533/534] Data 0.000 (0.004) Elapsed 5m 19s (remain 0m 0s) Loss: 0.0744(0.0734) Grad: 0.8270  \n",
      "EVAL: [0/134] Data 1.385 (1.385) Elapsed 0m 1s (remain 3m 23s) Loss: 0.0954(0.0954) \n",
      "EVAL: [100/134] Data 0.764 (0.225) Elapsed 0m 38s (remain 0m 12s) Loss: 0.0736(0.0736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0734  avg_val_loss: 0.0741  time: 369s\n",
      "Epoch 6 - Accuracy: 0.880607476635514\n",
      "Epoch 6 - Save Best Score: 0.8806 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.213) Elapsed 0m 48s (remain 0m 0s) Loss: 0.1337(0.0741) \n",
      "Epoch: [7][0/534] Data 1.891 (1.891) Elapsed 0m 2s (remain 22m 36s) Loss: 0.0800(0.0800) Grad: 0.5425  \n",
      "Epoch: [7][100/534] Data 0.000 (0.019) Elapsed 1m 2s (remain 4m 28s) Loss: 0.1016(0.0673) Grad: 0.9042  \n",
      "Epoch: [7][200/534] Data 0.000 (0.010) Elapsed 2m 1s (remain 3m 21s) Loss: 0.0962(0.0688) Grad: 0.7449  \n",
      "Epoch: [7][300/534] Data 0.003 (0.007) Elapsed 3m 1s (remain 2m 20s) Loss: 0.0311(0.0686) Grad: 0.5540  \n",
      "Epoch: [7][400/534] Data 0.000 (0.005) Elapsed 4m 0s (remain 1m 19s) Loss: 0.0711(0.0689) Grad: 0.7482  \n",
      "Epoch: [7][500/534] Data 0.000 (0.004) Elapsed 5m 0s (remain 0m 19s) Loss: 0.0479(0.0693) Grad: 0.7920  \n",
      "Epoch: [7][533/534] Data 0.000 (0.004) Elapsed 5m 19s (remain 0m 0s) Loss: 0.0543(0.0689) Grad: 0.5413  \n",
      "EVAL: [0/134] Data 1.319 (1.319) Elapsed 0m 1s (remain 3m 15s) Loss: 0.0892(0.0892) \n",
      "EVAL: [100/134] Data 0.000 (0.216) Elapsed 0m 37s (remain 0m 12s) Loss: 0.0685(0.0741) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0689  avg_val_loss: 0.0745  time: 368s\n",
      "Epoch 7 - Accuracy: 0.8789719626168224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.086 (0.212) Elapsed 0m 48s (remain 0m 0s) Loss: 0.1334(0.0745) \n",
      "Epoch: [8][0/534] Data 1.800 (1.800) Elapsed 0m 2s (remain 21m 50s) Loss: 0.0582(0.0582) Grad: 0.6563  \n",
      "Epoch: [8][100/534] Data 0.000 (0.019) Elapsed 1m 2s (remain 4m 28s) Loss: 0.0621(0.0669) Grad: 0.8415  \n",
      "Epoch: [8][200/534] Data 0.005 (0.010) Elapsed 2m 2s (remain 3m 22s) Loss: 0.0508(0.0675) Grad: 0.5527  \n",
      "Epoch: [8][300/534] Data 0.000 (0.007) Elapsed 3m 1s (remain 2m 20s) Loss: 0.0514(0.0660) Grad: 0.6277  \n",
      "Epoch: [8][400/534] Data 0.000 (0.005) Elapsed 4m 0s (remain 1m 19s) Loss: 0.0908(0.0666) Grad: 0.6926  \n",
      "Epoch: [8][500/534] Data 0.005 (0.004) Elapsed 5m 0s (remain 0m 19s) Loss: 0.0617(0.0666) Grad: 0.9075  \n",
      "Epoch: [8][533/534] Data 0.000 (0.004) Elapsed 5m 19s (remain 0m 0s) Loss: 0.0689(0.0666) Grad: 0.7719  \n",
      "EVAL: [0/134] Data 1.470 (1.470) Elapsed 0m 1s (remain 3m 38s) Loss: 0.0941(0.0941) \n",
      "EVAL: [100/134] Data 0.736 (0.225) Elapsed 0m 38s (remain 0m 12s) Loss: 0.0750(0.0708) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.0666  avg_val_loss: 0.0710  time: 369s\n",
      "Epoch 8 - Accuracy: 0.880607476635514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.214) Elapsed 0m 49s (remain 0m 0s) Loss: 0.1406(0.0710) \n",
      "Epoch: [9][0/534] Data 1.686 (1.686) Elapsed 0m 2s (remain 21m 10s) Loss: 0.0740(0.0740) Grad: 0.8147  \n",
      "Epoch: [9][100/534] Data 0.000 (0.017) Elapsed 1m 2s (remain 4m 28s) Loss: 0.0602(0.0645) Grad: 0.6171  \n",
      "Epoch: [9][200/534] Data 0.000 (0.009) Elapsed 2m 1s (remain 3m 22s) Loss: 0.0335(0.0640) Grad: 0.4622  \n",
      "Epoch: [9][300/534] Data 0.000 (0.006) Elapsed 3m 1s (remain 2m 20s) Loss: 0.0530(0.0634) Grad: 0.6419  \n",
      "Epoch: [9][400/534] Data 0.000 (0.005) Elapsed 4m 0s (remain 1m 19s) Loss: 0.0386(0.0630) Grad: 0.6531  \n",
      "Epoch: [9][500/534] Data 0.000 (0.004) Elapsed 5m 0s (remain 0m 19s) Loss: 0.0742(0.0631) Grad: 0.7997  \n",
      "Epoch: [9][533/534] Data 0.000 (0.004) Elapsed 5m 19s (remain 0m 0s) Loss: 0.0604(0.0629) Grad: 0.6158  \n",
      "EVAL: [0/134] Data 1.556 (1.556) Elapsed 0m 1s (remain 3m 46s) Loss: 0.0909(0.0909) \n",
      "EVAL: [100/134] Data 0.913 (0.228) Elapsed 0m 38s (remain 0m 12s) Loss: 0.0666(0.0703) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.0629  avg_val_loss: 0.0707  time: 369s\n",
      "Epoch 9 - Accuracy: 0.8841121495327103\n",
      "Epoch 9 - Save Best Score: 0.8841 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.218) Elapsed 0m 49s (remain 0m 0s) Loss: 0.1307(0.0707) \n",
      "Epoch: [10][0/534] Data 1.798 (1.798) Elapsed 0m 2s (remain 22m 9s) Loss: 0.0591(0.0591) Grad: 0.7369  \n",
      "Epoch: [10][100/534] Data 0.000 (0.019) Elapsed 1m 3s (remain 4m 30s) Loss: 0.0636(0.0636) Grad: 0.7908  \n",
      "Epoch: [10][200/534] Data 0.000 (0.010) Elapsed 2m 2s (remain 3m 23s) Loss: 0.0841(0.0622) Grad: 0.8928  \n",
      "Epoch: [10][300/534] Data 0.005 (0.007) Elapsed 3m 2s (remain 2m 21s) Loss: 0.0751(0.0623) Grad: 0.7309  \n",
      "Epoch: [10][400/534] Data 0.000 (0.005) Elapsed 4m 1s (remain 1m 20s) Loss: 0.0306(0.0621) Grad: 0.5439  \n",
      "Epoch: [10][500/534] Data 0.000 (0.004) Elapsed 5m 1s (remain 0m 19s) Loss: 0.0473(0.0618) Grad: 0.7679  \n",
      "Epoch: [10][533/534] Data 0.000 (0.004) Elapsed 5m 21s (remain 0m 0s) Loss: 0.0540(0.0620) Grad: 0.8300  \n",
      "EVAL: [0/134] Data 1.382 (1.382) Elapsed 0m 1s (remain 3m 26s) Loss: 0.0904(0.0904) \n",
      "EVAL: [100/134] Data 0.825 (0.225) Elapsed 0m 38s (remain 0m 12s) Loss: 0.0681(0.0694) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.0620  avg_val_loss: 0.0698  time: 371s\n",
      "Epoch 10 - Accuracy: 0.8827102803738318\n",
      "========== fold: 0 result ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [133/134] Data 0.000 (0.218) Elapsed 0m 49s (remain 0m 0s) Loss: 0.1321(0.0698) \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 3717.916006,
   "end_time": "2021-03-29T03:05:55.261888",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-29T02:03:57.345882",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
